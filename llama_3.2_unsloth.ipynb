{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nE451MrgkBq"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab and Kaggle notebooks! Otherwise use pip install unsloth\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
        "    !pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "    !pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "chYyadaEwjSD",
        "outputId": "6412645f-0a76-4eb5-f5e4-7f0fe0c560b3"
      },
      "outputs": [],
      "source": [
        "import unsloth\n",
        "from unsloth import FastVisionModel  # FastLanguageModel for LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XA5e_XLsRa7n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = False\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "from transformers import TextStreamer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6QAG0_e1oiLi"
      },
      "outputs": [],
      "source": [
        "PROJECT_DIR = \"nexcar-challenge\"\n",
        "DATA_DIR = \"data\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WCe3vc8NQ4T3"
      },
      "outputs": [],
      "source": [
        "%pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3K-FLq-oRnf"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp /content/kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aW0klbtKoTKM"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions download -c nexar-collision-prediction -p /content/{PROJECT_DIR}/{DATA_DIR}/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLfGP7KiqCIL"
      },
      "outputs": [],
      "source": [
        "!unzip /content/{PROJECT_DIR}/{DATA_DIR}/nexar-collision-prediction.zip -d /content/{PROJECT_DIR}/{DATA_DIR}/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTqa0RyZestR"
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "SECONDS_BEFORE = 0.5  # number of seconds before the collision for image frame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nd-sA9xeRZTK"
      },
      "outputs": [],
      "source": [
        "train_csv_path = f\"/content/{PROJECT_DIR}/{DATA_DIR}/train.csv\"\n",
        "train_videos_folder = f\"/content/{PROJECT_DIR}/{DATA_DIR}/train/\"\n",
        "\n",
        "df = pd.read_csv(train_csv_path)\n",
        "# df = df[df['target']==1]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQQqE3DGSkC3"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, data, videos_folder, transform=None):\n",
        "        self.data = data\n",
        "        self.videos_folder = videos_folder\n",
        "        self.transform = transform  # Any image transformations (e.g., augmentations)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "\n",
        "        vid_path = os.path.join(\n",
        "            self.videos_folder, f\"{str(int(row['id'])).zfill(5)}.mp4\"\n",
        "        )\n",
        "\n",
        "        time = (\n",
        "            0.0\n",
        "            if np.isnan(row[\"time_of_event\"])\n",
        "            else row[\"time_of_event\"] - SECONDS_BEFORE\n",
        "        )\n",
        "        image = self.get_frame(vid_path, time)\n",
        "\n",
        "        label = row[\"target\"]\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, torch.tensor(label), row['id']  # Convert label to tensor\n",
        "\n",
        "    def get_frame(self, video_path, time_sec):\n",
        "        cap = cv2.VideoCapture(video_path)\n",
        "        cap.set(cv2.CAP_PROP_POS_MSEC, time_sec * 1000)\n",
        "\n",
        "        success, frame = cap.read()\n",
        "        cap.release()\n",
        "\n",
        "        if not success:\n",
        "            raise ValueError(f\"Failed to read frame at {time_sec} seconds.\")\n",
        "\n",
        "        # Convert BGR to RGB\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        return Image.fromarray(frame)\n",
        "\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((480, 854)),  # Resize to a standard size\n",
        "    ]\n",
        ")\n",
        "\n",
        "dataset = ImageDataset(df, train_videos_folder, transform=transform)\n",
        "dataloader = DataLoader(dataset, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbEe9uKZZRMk"
      },
      "outputs": [],
      "source": [
        "image, label, _ = dataset[0]\n",
        "display(image)\n",
        "print(label)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pnNHm2dMhbre"
      },
      "outputs": [],
      "source": [
        "instruction = \"\"\"Examine the provided dashcam image and analyze the positions and trajectories of all visible vehicles and obstacles. Based solely on the visual cues in the image, determine if there is an imminent risk of collision. Answer with only \"Yes\" or \"No\".\"\"\"\n",
        "\n",
        "\n",
        "def convert_to_conversation(image, label):\n",
        "    conversation = [\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": instruction},\n",
        "                {\"type\": \"image\", \"image\": image},\n",
        "            ],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": \"Yes\" if label else \"No\"}],\n",
        "        },\n",
        "    ]\n",
        "    return {\"messages\": conversation}\n",
        "\n",
        "\n",
        "converted_dataset = [\n",
        "    convert_to_conversation(image, label) for image, label, _ in tqdm(dataset)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7neml1tkjaC1"
      },
      "outputs": [],
      "source": [
        "converted_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCXd-L23g8oX"
      },
      "outputs": [],
      "source": [
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Llama-3.2-11B-Vision-Instruct-bnb-4bit\",  # Llama 3.2 vision support\n",
        "    \"unsloth/Llama-3.2-11B-Vision-bnb-4bit\",\n",
        "    \"unsloth/Llama-3.2-90B-Vision-Instruct-bnb-4bit\",  # Can fit in a 80GB card!\n",
        "    \"unsloth/Llama-3.2-90B-Vision-bnb-4bit\",\n",
        "    \"unsloth/Pixtral-12B-2409-bnb-4bit\",  # Pixtral fits in 16GB!\n",
        "    \"unsloth/Pixtral-12B-Base-2409-bnb-4bit\",  # Pixtral base model\n",
        "    \"unsloth/Qwen2-VL-2B-Instruct-bnb-4bit\",  # Qwen2 VL support\n",
        "    \"unsloth/Qwen2-VL-7B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Qwen2-VL-72B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/llava-v1.6-mistral-7b-hf-bnb-4bit\",  # Any Llava variant works!\n",
        "    \"unsloth/llava-1.5-7b-hf-bnb-4bit\",\n",
        "]  # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastVisionModel.from_pretrained(\n",
        "    # \"unsloth/Llama-3.2-11B-Vision-Instruct\",\n",
        "    fourbit_models[0],\n",
        "    load_in_4bit=True,  # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
        "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for long context\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fCUKOZzghPt-"
      },
      "outputs": [],
      "source": [
        "model = FastVisionModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers=True,  # False if not finetuning vision layers\n",
        "    finetune_language_layers=True,  # False if not finetuning language layers\n",
        "    finetune_attention_modules=True,  # False if not finetuning attention layers\n",
        "    finetune_mlp_modules=True,  # False if not finetuning MLP layers\n",
        "    r=16,  # The larger, the higher the accuracy, but might overfit\n",
        "    lora_alpha=16,  # Recommended alpha == r at least\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    random_state=3407,\n",
        "    use_rslora=False,  # We support rank stabilized LoRA\n",
        "    loftq_config=None,  # And LoftQ\n",
        "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmAXtdjcXf9k"
      },
      "outputs": [],
      "source": [
        "FastVisionModel.for_inference(model)  # Enable for inference!\n",
        "\n",
        "image, label, _ = dataset[0]\n",
        "conversation = convert_to_conversation(image, label)\n",
        "\n",
        "input_text = tokenizer.apply_chat_template(\n",
        "    conversation[\"messages\"], add_generation_prompt=True\n",
        ")\n",
        "inputs = tokenizer(\n",
        "    image,\n",
        "    input_text,\n",
        "    add_special_tokens=False,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "_ = model.generate(\n",
        "    **inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=128,\n",
        "    use_cache=True,\n",
        "    temperature=1.5,\n",
        "    min_p=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6rs6DdTiinw"
      },
      "outputs": [],
      "source": [
        "from unsloth import is_bf16_supported\n",
        "from unsloth.trainer import UnslothVisionDataCollator\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "FastVisionModel.for_training(model)  # Enable for training!\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=UnslothVisionDataCollator(model, tokenizer),  # Must use!\n",
        "    train_dataset=converted_dataset,\n",
        "    args=SFTConfig(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        max_steps=30,\n",
        "        # num_train_epochs = 1, # Set this instead of max_steps for full training runs\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not is_bf16_supported(),\n",
        "        bf16=is_bf16_supported(),\n",
        "        logging_steps=1,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=\"outputs\",\n",
        "        report_to=\"none\",  # For Weights and Biases\n",
        "        # You MUST put the below items for vision finetuning:\n",
        "        remove_unused_columns=False,\n",
        "        dataset_text_field=\"\",\n",
        "        dataset_kwargs={\"skip_prepare_dataset\": True},\n",
        "        dataset_num_proc=4,\n",
        "        max_seq_length=2048,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIPjcnI6itt5"
      },
      "outputs": [],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4633KabSivty"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LFwJX-bizA7"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZS8fUqIi2w3"
      },
      "outputs": [],
      "source": [
        "FastVisionModel.for_inference(model)  # Enable for inference!\n",
        "\n",
        "image, label, _ = dataset[0]\n",
        "conversation = convert_to_conversation(image, label)\n",
        "\n",
        "input_text = tokenizer.apply_chat_template(\n",
        "    conversation[\"messages\"], add_generation_prompt=True\n",
        ")\n",
        "inputs = tokenizer(\n",
        "    image,\n",
        "    input_text,\n",
        "    add_special_tokens=False,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "_ = model.generate(\n",
        "    **inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=128,\n",
        "    use_cache=True,\n",
        "    temperature=1.5,\n",
        "    min_p=0.1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPx_ZPbot9pX"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\n",
        "    f\"/content/drive/MyDrive/{PROJECT_DIR}/pretrained/finetuned_llama/\"\n",
        ")\n",
        "tokenizer.save_pretrained(\n",
        "    f\"/content/drive/MyDrive/{PROJECT_DIR}/pretrained/finetuned_llama/\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7g6ue-UXud6K"
      },
      "outputs": [],
      "source": [
        "correct = 0\n",
        "total = len(dataset)\n",
        "\n",
        "for i, (image, target, _) in enumerate(tqdm(dataset)):\n",
        "    conversation = convert_to_conversation(image, label)\n",
        "    input_text = tokenizer.apply_chat_template(\n",
        "        conversation[\"messages\"], add_generation_prompt=True\n",
        "    )\n",
        "    inputs = tokenizer(\n",
        "        image,\n",
        "        input_text,\n",
        "        add_special_tokens=False,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # Generate output and decode text\n",
        "    output_tokens = model.generate(\n",
        "        **inputs, max_new_tokens=128, use_cache=True, temperature=1.5, min_p=0.1\n",
        "    )\n",
        "    output_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True).lower()\n",
        "\n",
        "    # Evaluate the output:\n",
        "    if (target == 1 and \"yes\" in output_text.lower()) or (\n",
        "        target != 1 and \"no\" in output_text.lower()\n",
        "    ):\n",
        "        correct += 1\n",
        "\n",
        "accuracy = correct / total\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GotHZBmx-qI"
      },
      "outputs": [],
      "source": [
        "test_csv_path = f\"/content/drive/MyDrive/{PROJECT_DIR}/{DATA_DIR}/test.csv\"\n",
        "test_videos_folder = f\"/content/drive/MyDrive/{PROJECT_DIR}/{DATA_DIR}/test/\"\n",
        "test_set = ImageDataset(df, train_videos_folder, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2Wkw9Kcyknj"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "\n",
        "for image, target, id in test_set:\n",
        "    conversation = convert_to_conversation(image, label)\n",
        "    input_text = tokenizer.apply_chat_template(\n",
        "        conversation[\"messages\"], add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    # Tokenize the image and text inputs and move them to CUDA.\n",
        "    inputs = tokenizer(\n",
        "        image,\n",
        "        input_text,\n",
        "        add_special_tokens=False,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    output_tokens = model.generate(\n",
        "        **inputs, max_new_tokens=128, use_cache=True, temperature=1.5, min_p=0.1\n",
        "    )\n",
        "\n",
        "    target = 1 if \"yes\" in output_text.lower() else 0\n",
        "    results.append({\"id\": id, \"target\": target})\n",
        "\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv(\"test_results.csv\", index=False)\n",
        "print(\"Test set results saved to 'test_results.csv'\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "KAGGLE",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
