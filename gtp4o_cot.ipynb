{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import openai\n",
    "import base64\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "# Hyperparameters\n",
    "NUM_FRAMES = 5  # Number of frames to extract from the video\n",
    "TIME_BETWEEN_FRAMES = 1  # Time between frames in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_path = f\"{DATA_DIR}/train.csv\"\n",
    "train_videos_folder = f\"{DATA_DIR}/train/\"\n",
    "\n",
    "df = pd.read_csv(train_csv_path)\n",
    "# df = df[df['target']==1]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data, videos_folder, transform=None):\n",
    "        self.data = data\n",
    "        self.videos_folder = videos_folder\n",
    "        self.transform = transform  # Any image transformations (e.g., augmentations)\n",
    "        self.num_frames = NUM_FRAMES\n",
    "        self.time_between_frames = TIME_BETWEEN_FRAMES\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        vid_path = os.path.join(\n",
    "            self.videos_folder, f\"{str(int(row['id'])).zfill(5)}.mp4\"\n",
    "        )\n",
    "\n",
    "        time = 0 if np.isnan(row[\"time_of_event\"]) else row[\"time_of_event\"]\n",
    "        images = self.get_multiple_frames(vid_path, time)\n",
    "\n",
    "        label = row[\"target\"]\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            images = [self.transform(image) for image in images]\n",
    "\n",
    "        return images, torch.tensor(label), row[\"id\"]  # Convert label to tensor\n",
    "\n",
    "    def get_frame(self, video_path, time):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        cap.set(cv2.CAP_PROP_POS_MSEC, time * 1000)\n",
    "\n",
    "        success, frame = cap.read()\n",
    "        cap.release()\n",
    "\n",
    "        if not success:\n",
    "            raise ValueError(f\"Failed to read frame at {time} seconds.\")\n",
    "\n",
    "        # Convert BGR to RGB\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        return Image.fromarray(frame)\n",
    "\n",
    "    def get_multiple_frames(self, video_path, time):\n",
    "        time_between_frames = self.time_between_frames\n",
    "\n",
    "        if time == 0:\n",
    "            # Get the first frame\n",
    "            frame = self.get_frame(video_path, time)\n",
    "            return [frame]\n",
    "\n",
    "        if time is None:\n",
    "            # Get the last frames\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            cap.set(cv2.CAP_PROP_POS_AVI_RATIO, 1)\n",
    "            time = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000\n",
    "            time_between_frames = time / self.num_frames\n",
    "            cap.release()\n",
    "\n",
    "        frames = []\n",
    "        for i in range(self.num_frames):\n",
    "            try:\n",
    "                frame = self.get_frame(video_path, time - i * time_between_frames)\n",
    "                frames.append(frame)\n",
    "            except ValueError:\n",
    "                break\n",
    "        return frames\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((480, 854)),  # Resize to a standard size\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = ImageDataset(df, train_videos_folder, transform=transform)\n",
    "dataloader = DataLoader(dataset, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, label, id = dataset[np.random.randint(0, len(dataset))]\n",
    "\n",
    "num_images = len(images)\n",
    "fig, axes = plt.subplots(1, num_images, figsize=(24, 3))\n",
    "\n",
    "if num_images == 1:\n",
    "    axes.imshow(images[0])\n",
    "    axes.axis(\"off\")\n",
    "else:\n",
    "    for i, image in enumerate(images):\n",
    "        axes[i].imshow(image)\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"Label: {int(label)}, ID: {int(id)}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_prompt(label):\n",
    "    instruction = f\"\"\"\n",
    "    You are an expert in accident reconstruction and traffic analysis. You will analyze a sequence of dashcam images with a chain of thought reasoning to determine whether there is an immediate threat of vehicle collision. In this particular scenario, there was {\"a collision\" if label else \"no collision\"}. Consider each of the following factors:\n",
    "\n",
    "    1. Vehicle Positions: Identify the locations of all vehicles in each frame and how they change over time.\n",
    "    2. Trajectories: Determine the direction, speed, and acceleration of each vehicle by comparing their positions across frames.\n",
    "    3. Nearby Vehicles and Traffic: Identify surrounding vehicles, pedestrians, and any traffic congestion that could impact movement.\n",
    "    4. Traffic Signals: Consider whether traffic signals indicate a stop, go, or caution state and how that affects the vehicle interactions. Pay special attention on whether vehicles are vialating or obeying traffic signal rules.\n",
    "    5. Road Conditions and Visibility: Note any obstructions, road markings, or weather conditions that could contribute to the situation.\n",
    "    \"\"\"\n",
    "\n",
    "    return instruction\n",
    "\n",
    "\n",
    "def encode_image(image):\n",
    "    buffered = io.BytesIO()\n",
    "    image.save(buffered, format=\"JPEG\")\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "def convert_to_conversation(images, label):\n",
    "    # Combine all images into a single conversation\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": get_llm_prompt(int(label))},\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                *[\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\n",
    "                            \"url\": f\"data:image/jpeg;base64,{encode_image(img)}\",\n",
    "                        },\n",
    "                    }\n",
    "                    for img in images\n",
    "                ],\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "def generate_response(conversation):\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=conversation,\n",
    "        n=1,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "\n",
    "with open(f\"gpt4o_cot.csv\", mode=\"w\", newline=\"\") as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\"id\", \"label\", \"response\"])\n",
    "    writer.writeheader()\n",
    "\n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        images, label, id = dataset[i]\n",
    "        conversation = convert_to_conversation(images, label)\n",
    "        response = generate_response(conversation)\n",
    "        response_data = {\"id\": int(id), \"label\": int(label), \"response\": response}\n",
    "        responses.append(response_data)\n",
    "        writer.writerow(response_data)\n",
    "\n",
    "responses_df = pd.DataFrame(responses)\n",
    "responses_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
