{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from transformers import TextStreamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "SECONDS_BEFORE = 0.5  # number of seconds before the collision for image frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_DIR = \"nexcar-challenge\"\n",
    "DATA_DIR = \"data\"\n",
    "\n",
    "train_csv_path = f\"/content/drive/MyDrive/{PROJECT_DIR}/{DATA_DIR}/train.csv\"\n",
    "train_videos_folder = f\"/content/drive/MyDrive/{PROJECT_DIR}/{DATA_DIR}/train/\"\n",
    "\n",
    "df = pd.read_csv(train_csv_path)\n",
    "# df = df[df['target']==1]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data, videos_folder, transform=None):\n",
    "        self.data = data\n",
    "        self.videos_folder = videos_folder\n",
    "        self.transform = transform  # Any image transformations (e.g., augmentations)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        vid_path = os.path.join(\n",
    "            self.videos_folder, f\"{str(int(row['id'])).zfill(5)}.mp4\"\n",
    "        )\n",
    "\n",
    "        time = (\n",
    "            0.0\n",
    "            if np.isnan(row[\"time_of_event\"])\n",
    "            else row[\"time_of_event\"] - SECONDS_BEFORE\n",
    "        )\n",
    "        image = self.get_frame(vid_path, time)\n",
    "\n",
    "        label = row[\"target\"]\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label), row['id']  # Convert label to tensor\n",
    "\n",
    "    def get_frame(self, video_path, time_sec):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        cap.set(cv2.CAP_PROP_POS_MSEC, time_sec * 1000)\n",
    "\n",
    "        success, frame = cap.read()\n",
    "        cap.release()\n",
    "\n",
    "        if not success:\n",
    "            raise ValueError(f\"Failed to read frame at {time_sec} seconds.\")\n",
    "\n",
    "        # Convert BGR to RGB\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        return Image.fromarray(frame)\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((480, 854)),  # Resize to a standard size\n",
    "    ]\n",
    ")\n",
    "\n",
    "dataset = ImageDataset(df, train_videos_folder, transform=transform)\n",
    "dataloader = DataLoader(dataset, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label, _ = dataset[0]\n",
    "display(image)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"Examine the provided dashcam image and analyze the positions and trajectories of all visible vehicles and obstacles. Based solely on the visual cues in the image, determine if there is an imminent risk of collision. Answer with only \"Yes\" or \"No\".\"\"\"\n",
    "\n",
    "\n",
    "def convert_to_conversation(image, label):\n",
    "    conversation = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": instruction},\n",
    "                {\"type\": \"image\", \"image\": image},\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"Yes\" if label else \"No\"}],\n",
    "        },\n",
    "    ]\n",
    "    return {\"messages\": conversation}\n",
    "\n",
    "\n",
    "converted_dataset = [\n",
    "    convert_to_conversation(image, label) for image, label, _ in tqdm(dataset)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab and Kaggle notebooks! Otherwise use pip install unsloth\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
    "    !pip install --no-deps cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "    !pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastVisionModel  # FastLanguageModel for LLMs\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastVisionModel.from_pretrained(\n",
    "    f\"/content/drive/MyDrive/{PROJECT_DIR}/pretrained/finetuned_llama/\",\n",
    "    load_in_4bit=True,  # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
    "    use_gradient_checkpointing=\"unsloth\",  # True or \"unsloth\" for long context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastVisionModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers=True,  # False if not finetuning vision layers\n",
    "    finetune_language_layers=True,  # False if not finetuning language layers\n",
    "    finetune_attention_modules=True,  # False if not finetuning attention layers\n",
    "    finetune_mlp_modules=True,  # False if not finetuning MLP layers\n",
    "    r=16,  # The larger, the higher the accuracy, but might overfit\n",
    "    lora_alpha=16,  # Recommended alpha == r at least\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  # We support rank stabilized LoRA\n",
    "    loftq_config=None,  # And LoftQ\n",
    "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastVisionModel.for_inference(model)  # Enable for inference!\n",
    "\n",
    "image, label, _ = dataset[0]\n",
    "conversation = convert_to_conversation(image, label)\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(\n",
    "    conversation[\"messages\"], add_generation_prompt=True\n",
    ")\n",
    "inputs = tokenizer(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=128,\n",
    "    use_cache=True,\n",
    "    temperature=1.5,\n",
    "    min_p=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastVisionModel.for_inference(model)  # Enable for inference!\n",
    "\n",
    "image, label, _ = dataset[0]\n",
    "conversation = convert_to_conversation(image, label)\n",
    "\n",
    "input_text = tokenizer.apply_chat_template(\n",
    "    conversation[\"messages\"], add_generation_prompt=True\n",
    ")\n",
    "inputs = tokenizer(\n",
    "    image,\n",
    "    input_text,\n",
    "    add_special_tokens=False,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=128,\n",
    "    use_cache=True,\n",
    "    temperature=1.5,\n",
    "    min_p=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = len(dataset)\n",
    "\n",
    "for i, (image, target, _) in enumerate(tqdm(dataset)):\n",
    "    conversation = convert_to_conversation(image, label)\n",
    "    input_text = tokenizer.apply_chat_template(\n",
    "        conversation[\"messages\"], add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(\n",
    "        image,\n",
    "        input_text,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    # Generate output and decode text\n",
    "    output_tokens = model.generate(\n",
    "        **inputs, max_new_tokens=128, use_cache=True, temperature=1.5, min_p=0.1\n",
    "    )\n",
    "    output_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True).lower()\n",
    "\n",
    "    # Evaluate the output:\n",
    "    if (target == 1 and \"yes\" in output_text.lower()) or (\n",
    "        target != 1 and \"no\" in output_text.lower()\n",
    "    ):\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / total\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv_path = f\"/content/drive/MyDrive/{PROJECT_DIR}/{DATA_DIR}/test.csv\"\n",
    "test_videos_folder = f\"/content/drive/MyDrive/{PROJECT_DIR}/{DATA_DIR}/test/\"\n",
    "test_set = ImageDataset(df, train_videos_folder, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for image, target, id in test_set:\n",
    "    conversation = convert_to_conversation(image, label)\n",
    "    input_text = tokenizer.apply_chat_template(\n",
    "        conversation[\"messages\"], add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    # Tokenize the image and text inputs and move them to CUDA.\n",
    "    inputs = tokenizer(\n",
    "        image,\n",
    "        input_text,\n",
    "        add_special_tokens=False,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    output_tokens = model.generate(\n",
    "        **inputs, max_new_tokens=128, use_cache=True, temperature=1.5, min_p=0.1\n",
    "    )\n",
    "\n",
    "    target = 1 if \"yes\" in output_text.lower() else 0\n",
    "    results.append({\"id\": id, \"target\": target})\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(\"test_results.csv\", index=False)\n",
    "print(\"Test set results saved to 'test_results.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
